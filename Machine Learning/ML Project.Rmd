# Human Activity Recognition

## Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

The goal of this project is to predict the manner in which they did the exercise. The data was collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. (see the section on the Weight Lifting Exercise Dataset). 


## Exploratory analysis

The data consists of 

  a) a training set of 19622 observations with 160 variables
  
      https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
  
  b) a testing set of 20 observations with 160 variables
  
      https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
      
  The outcome we want to predict is the variable "classe", with possible values A/B/C/D/E. Please refer to the summary in Output 1.2 and 1.3
  
  Please refer to Output 1.4 for the structure of the training data. All the data has been initially loaded as characters but most of the variables in the training data set are numeric in nature. Also a lot of variables are sparsely populated - see some examples in Output 1.5
  
## Models

Predictive models were created as listed below. Please refer to Output 2.1 for the relevant r code.

1. The following variables are not directly related to the accelerometer readings. They were not included in the model.

  user_name
  cvtd_timestamp
  classe
  X
  raw_timestamp_part_1
  raw_timestamp_part_2
  new_window
  
2. From the remaining variables all the sparsely populated variables (that have less than 50% of data) were also excluded from the model.

3. The training data was split into true training (15%) and cross-validation data sets (85%)

4. The following two methods were applied to the true training data set

  a) random forests 
  
  b) gradient boosing

5. The two models were then used to predict the outcome of the observations in the cross-validation data set as well as the testing data set.

6. The accuracy was measured by comparing the predicted values with the actual values in the cross-validation data set.

7. These steps were repeted a few times to get the average out of sample error Output 2.2



## Conclusions & Out of sample error

The accuracy and out of sample error for the two models considered were

Random forests      - Accuracy = 94.5% ; Out of sample error = 5.5%
Gradient boosing    - Accuracy = 93.4% ; Out of sample error = 6.6%

The prediction accuracy on the 20 test observations was 100%

## Appendix


========================================================

### Output 1.1 - Load the libraries and files
```{r}
# load the libraries
library (caret)
library (randomForest)
library (gbm)
library (survival)
library (splines)
library (plyr)

# set training sample percentage
trSamplePct = 0.05

set.seed (233)

# load training and test data files

training = read.csv ("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", colClasses = "character")
testing = read.csv ("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", colClasses = "character")

```

### Output 1.2 - Summary of the variable "classe" 

```{r}        
summary (as.factor(training$classe))
```

### Output 1.3 - Barplot of the variable "classe" 

```{r}
barplot (summary(as.factor(training$classe)))
```

### Output 1.4 - Structure of the training data set

```{r}
str (training)
```

### Output 1.5 - Examples of sparsely populated variables in the training data set

```{r}
summary (suppressWarnings(as.numeric (training$kurtosis_roll_belt)))

summary (suppressWarnings(as.numeric (training$skewness_roll_belt)))

```

### Output 2.1 - Models

```{r}
# this function cleans up the training and testing data sets and prepares them for model fitting
cleanupData = function (df) {
  
  # create a temp copy of df
  dfTmp = df
  
  # remove all the variables that are not going to be used in the model
  dfTmp$user_name = NULL
  dfTmp$cvtd_timestamp = NULL
  dfTmp$classe = NULL
  dfTmp$X = NULL
  dfTmp$raw_timestamp_part_1 = NULL
  dfTmp$raw_timestamp_part_2 = NULL
  dfTmp$new_window = NULL
  
  
  # convert all the variables with numerical data to numeric data types
  # use suppressWarnings () to avoid the warning "NAs introduced by coercion"
  dfTmp [sapply(dfTmp, is.character)] = 
    suppressWarnings(lapply (dfTmp [sapply (dfTmp, is.character)], as.numeric))
    
  # combine all the variables to form a clean dataframe and return
  dfClean = cbind (classe = as.factor (df$classe), dfTmp)
  
  return (dfClean)
}

# add missing variable classe to testing data set                    
testing$classe = "E"

ptm = proc.time ()

# cleanup the training and testing data sets
trainingClean = cleanupData (training)

testingClean = cleanupData (testing)
# remove the extra columns in testing
testingClean$problem_id = NULL

# Remove all columns that have less than a threshold percentage of data
thresh = 0.5

for (i in colnames (trainingClean)) {
  
  if (sum (!is.na (trainingClean [i]))/nrow(trainingClean) < thresh) {
    trainingClean [i] = NULL
    
    # remove the same from testing data as well
    testingClean [i] = NULL
  }
}

# create separate training and cross-validation data sets from the training data
inTrain = createDataPartition(trainingClean$classe, p = trSamplePct)[[1]]
trTrain = trainingClean [ inTrain,]
trTest = trainingClean [-inTrain,]

set.seed (62433)

## Model - 1
# fit a model with random forest using the train data
modRf = train (trTrain$classe ~ ., method = "rf", data = trTrain)

# predict using the cross-validation data created from training data
pRf = predict (modRf, newdata = trTest)

# predict using the test data
pRfTest = predict (modRf, newdata = testingClean)

## Model - 2
# fit a model with gbm using the train data
modGbm = train (trTrain$classe ~ ., method = "gbm", data = trTrain, verbose = FALSE)

# predict using the cross-validation data created from training data
pGbm = predict (modGbm, newdata = trTest)

# predict using the test data
pGbmTest = predict (modGbm, newdata = testingClean)


```

### Output 2.2 - Accuracy and Out of sample error

```{r}

# check the accuracy of the model on the cross-validation data
accuracyRf = (confusionMatrix(pRf, trTest$classe))$overall[1]

# check the accuracy of the model on the cross-validation data
accuracyGbm = (confusionMatrix(pGbm, trTest$classe))$overall[1]

# Out of sample error
print (paste0 ("Random forests - Out of sample error (%) = ", 
               round ((1 - accuracyRf) * 100, 1)))
       
print (paste0 ("Gradient Boosting - Out of sample error (%) = ", 
               round ((1 - accuracyGbm) * 100, 1)))

```